<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computational-Methods | Qingzhou Zhang</title>
    <link>https://zqzneptune.github.io/tags/computational-methods/</link>
      <atom:link href="https://zqzneptune.github.io/tags/computational-methods/index.xml" rel="self" type="application/rss+xml" />
    <description>Computational-Methods</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2014-2025 Qingzhou Zhang (CC) BY-NC-SA 4.0. All thoughts and opinions here are my own.</copyright><lastBuildDate>Wed, 03 Feb 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://zqzneptune.github.io/images/icon_huf504e8a2f7c5cf9e43cec507892894a0_49847_512x512_fill_lanczos_center_2.png</url>
      <title>Computational-Methods</title>
      <link>https://zqzneptune.github.io/tags/computational-methods/</link>
    </image>
    
    <item>
      <title>From What to Why: How Perturb-seq Rewrote the Rules of Functional Genomics</title>
      <link>https://zqzneptune.github.io/post/2021-02-03_perturbseqinit/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://zqzneptune.github.io/post/2021-02-03_perturbseqinit/</guid>
      <description>&lt;h2 id=&#34;the-core-finding-in-a-nutshell&#34;&gt;The Core Finding in a Nutshell&lt;/h2&gt;
&lt;p&gt;I’m taking a look back at the foundational 2016 paper from the Regev and Friedman labs that introduced Perturb-seq. At its core, they developed a method that welds a pooled CRISPR screen to single-cell RNA sequencing. For the first time, this allowed them to systematically knock out genes and read out the full downstream transcriptional consequences in thousands of individual cells at once, in a single experiment.&lt;/p&gt;
&lt;h2 id=&#34;the-problem-a-chasm-between-scale-and-depth&#34;&gt;The Problem: A Chasm Between Scale and Depth&lt;/h2&gt;
&lt;p&gt;Before this work, functional genomics was a frustrating trade-off. We could either go big with pooled screens that measured simple, one-dimensional outputs like cell survival, or go deep with low-throughput, one-gene-at-a-time transcriptomics. There was simply no way to causally link hundreds of genetic perturbations to high-dimensional phenotypes at scale. How do you build a causal map of a cell&amp;rsquo;s regulatory network if you can only see one intersection at a time? This paper provided the first practical roadmap to bridge that chasm.&lt;/p&gt;
&lt;h2 id=&#34;their-analytical-engine-a-linear-model-with-biological-awareness&#34;&gt;Their Analytical Engine: A Linear Model with Biological Awareness&lt;/h2&gt;
&lt;p&gt;The elegance of Perturb-seq isn&amp;rsquo;t just in the wet lab design. Computationally, they built a framework called MIMOSCA (Multi-Input-Multi-Output-Single-Cell-Analysis). At its heart, it&amp;rsquo;s a regularized linear model that attributes changes in each gene&amp;rsquo;s expression to the presence of a specific sgRNA. The real genius, however, is how they layered in covariates. The model doesn&amp;rsquo;t just capture the perturbation&amp;rsquo;s effect; it systematically models and subtracts the confounding effects of technical noise (like sequencing depth per cell) and, most importantly, pre-existing biological heterogeneity (like cell cycle or differentiation state). This layered approach allows them to isolate the true, direct impact of the gene knockout from other sources of variation.&lt;/p&gt;
&lt;h2 id=&#34;the-result-that-demands-attention&#34;&gt;The Result That Demands Attention&lt;/h2&gt;
&lt;p&gt;The most profound insight for me was their ability to disentangle direct transcriptional regulation from shifts in cell state proportions. When they first modeled the data from dendritic cells, the inferred regulatory network was a bit blurry (Fig 3A). The &amp;lsquo;aha!&amp;rsquo; moment came after they accounted for cell state. They realized that a transcription factor knockout might not create an entirely &lt;em&gt;new&lt;/em&gt; cellular program, but rather push cells from, say, a &amp;ldquo;pro-inflammatory&amp;rdquo; state to an &amp;ldquo;antiviral&amp;rdquo; state—both of which already exist in the unperturbed population. After computationally correcting for this cell state shifting, the direct regulatory links became razor-sharp (Fig 3E). This is a fundamental lesson: a perturbation&amp;rsquo;s effect isn&amp;rsquo;t always to create something novel, but often to change the balance of what&amp;rsquo;s already there. This is a critical concept for understanding drug resistance, where a therapy might simply select for a pre-existing resistant subclone rather than inducing a new program from scratch.&lt;/p&gt;
&lt;h2 id=&#34;how-this-informs-my-predictive-modeling-roadmap&#34;&gt;How This Informs My Predictive Modeling Roadmap&lt;/h2&gt;
&lt;p&gt;This paper is not just another publication; it is the technical and philosophical blueprint for my entire research program. My mission is to build causal, predictive models of tissues, and Perturb-seq is the engine that generates the necessary data to make that possible. My focus on youth leukemia chemoresistance and solid tumor immunology hinges on understanding how cancer cells rewire their networks to survive therapy.&lt;/p&gt;
&lt;p&gt;With the Perturb-seq framework, I can move beyond merely correlating gene expression with a resistant phenotype. I can now systematically knock out transcription factors, signaling molecules, or metabolic enzymes in a patient-derived model and directly observe which pathways are causally required for survival. This provides the raw, causal data needed to build a digital twin that doesn&amp;rsquo;t just describe the resistant state, but can &lt;em&gt;predict&lt;/em&gt; which nodes to target to reverse it. This paper provides the practical embodiment of my &amp;ldquo;Causality over Correlation&amp;rdquo; philosophy.&lt;/p&gt;
&lt;h2 id=&#34;the-horizon-my-next-move-and-the-fields-next-challenge&#34;&gt;The Horizon: My Next Move and the Field&amp;rsquo;s Next Challenge&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;My Next Computational Step:&lt;/strong&gt; The linear model in MIMOSCA was a brilliant and necessary starting point, but biology is rarely linear. My immediate goal is to build on this foundation by developing non-linear models, likely leveraging graph neural networks or variational autoencoders, to capture more complex, synergistic relationships between perturbed genes. The ultimate objective is to build a model that can predict the transcriptional outcome of a &lt;em&gt;combination&lt;/em&gt; of perturbations it has never seen before, moving from inference to true out-of-sample prediction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Experiment for the Field:&lt;/strong&gt; The next frontier is to take this technology from 2D cell culture into more complex, in-vivo like systems. A critical experiment would be to combine Perturb-seq with co-culture organoid models or even direct in-vivo delivery in mouse models of leukemia. Imagine perturbing every known kinase in leukemia cells and then observing how those perturbations causally alter not only the cancer cell&amp;rsquo;s transcriptome but also its communication with the surrounding bone marrow stroma. That is how we will begin to map the networks that drive therapeutic resistance in a truly relevant biological context.&lt;/p&gt;
&lt;h2 id=&#34;a-necessary-word-of-caution&#34;&gt;A Necessary Word of Caution&lt;/h2&gt;
&lt;p&gt;While this work is foundational, it&amp;rsquo;s important to view this 2016 paper through a modern lens. The reliance on gene knockout is a blunt instrument; we now have access to CRISPR interference (CRISPRi) and activation (CRISPRa) for more subtle modulation. The computational model also has to contend with imperfect guide capture and variable knockout efficiency, requiring statistical inference to decide which cells were &amp;ldquo;truly&amp;rdquo; perturbed. This introduces a potential source of modeling noise. Finally, the scale, while groundbreaking for its time, is now more routine. The core challenge they identified—deconvolving combinatorial perturbations—remains the key bottleneck, as the experimental possibility space grows exponentially. Their work laid the foundation, but scaling the analysis to true combinatorial complexity is still the grand challenge for the field.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Reference: &lt;a href=&#34;http://dx.doi.org/10.1016/j.cell.2016.11.038&#34;&gt;Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Busting the Myth of the scRNA-seq &#34;Dropout&#34;</title>
      <link>https://zqzneptune.github.io/post/2020-01-20-zerodrop/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://zqzneptune.github.io/post/2020-01-20-zerodrop/</guid>
      <description>&lt;h2 id=&#34;the-30-second-lowdown&#34;&gt;The 30-Second Lowdown&lt;/h2&gt;
&lt;p&gt;I love papers that challenge a core assumption we&amp;rsquo;ve all been working with. In this short but powerful correspondence, Valentine Svensson re-examines the widely held belief that droplet-based single-cell RNA-seq data has &amp;ldquo;too many&amp;rdquo; zeros due to technical failures, a phenomenon often called &amp;ldquo;dropout.&amp;rdquo; Using a series of clever analyses on negative control datasets, he demonstrates that the number of observed zeros is almost perfectly explained by standard statistical models of molecule counting. The punchline? The &amp;ldquo;excess&amp;rdquo; zeros we see in our biological data aren&amp;rsquo;t technical noise to be corrected; they are a reflection of real biology.&lt;/p&gt;
&lt;h2 id=&#34;the-ghost-in-the-machine-are-zeros-signal-or-noise&#34;&gt;The Ghost in the Machine: Are Zeros Signal or Noise?&lt;/h2&gt;
&lt;p&gt;A fundamental question in single-cell genomics has always been this: when my data matrix shows a zero for a given gene in a specific cell, what does it mean? Did the technology simply fail to capture an expressed transcript (a &amp;ldquo;technical zero&amp;rdquo;), or is that gene genuinely not being expressed (a &amp;ldquo;biological zero&amp;rdquo;)?&lt;/p&gt;
&lt;p&gt;This isn&amp;rsquo;t just an academic question—it dictates how we build our models. For years, the field has leaned heavily on the idea that technical zeros are rampant, especially in high-throughput droplet methods. This assumption has spawned an entire cottage industry of computational methods for &amp;ldquo;imputation&amp;rdquo;—algorithms designed to predict and fill in these supposed missing values. But if that foundational assumption is shaky, it means we might be spending our time &amp;ldquo;fixing&amp;rdquo; a problem that doesn&amp;rsquo;t exist, and potentially even corrupting the real biological signal in the process.&lt;/p&gt;
&lt;h2 id=&#34;a-clean-experiment-to-settle-the-score&#34;&gt;A Clean Experiment to Settle the Score&lt;/h2&gt;
&lt;p&gt;The beauty of this paper lies not in a complex new algorithm, but in its elegant application of statistical first principles to the right kind of data. Svensson leverages public negative control datasets where, instead of single cells, a uniform solution of RNA is encapsulated into droplets. This creates a perfect testbed: a system with zero biological variation, where any deviation from a simple sampling model must be technical.&lt;/p&gt;
&lt;p&gt;He then fits a standard count model—the negative binomial distribution—to this data. This model is a workhorse for count data and accounts for the fact that capturing molecules is a random process. The result is striking: the model perfectly predicts the observed fraction of zeros without needing any extra &amp;ldquo;zero-inflation&amp;rdquo; component. It’s a textbook case of Occam&amp;rsquo;s razor; the simplest model fits the technical data perfectly.&lt;/p&gt;
&lt;h2 id=&#34;the-finding-that-changes-things&#34;&gt;The Finding That Changes Things&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;aha!&amp;rsquo; moment for me is the stark visual contrast in Figure 1. When you look at the plots from the technical control experiments (panels a-e), the black dots (observed zero fraction) sit right on top of the gray line (the model&amp;rsquo;s prediction). There is no &amp;ldquo;excess&amp;rdquo; zero problem.&lt;/p&gt;
&lt;p&gt;Then, you look at the plots from real biological samples (panels f-h). Here, you see a clear deviation: many genes have more zeros than predicted by a simple model with a single &amp;ldquo;dispersion&amp;rdquo; parameter for all genes. This is the deviation that has fueled the dropout narrative. But the author shows that once you allow each gene to have its own biological variability (a gene-wise dispersion), the model once again explains the data much better. The takeaway is unambiguous: the platform itself isn&amp;rsquo;t systematically failing. The extra zeros come from biological heterogeneity. Some cells express a gene, others don&amp;rsquo;t. That’s biology, not a bug.&lt;/p&gt;
&lt;h2 id=&#34;why-this-matters-for-building-predictive-models&#34;&gt;Why This Matters for Building Predictive Models&lt;/h2&gt;
&lt;p&gt;This paper lands squarely on my intellectual home turf. My entire research program is geared towards building interpretable, predictive models of living tissues, and that starts with respecting the data. For years, we&amp;rsquo;ve seen increasingly complex imputation models, many of which are black boxes that fundamentally alter the raw counts before any biological questions are asked. This work provides strong evidence that this entire preprocessing step might be a misguided effort.&lt;/p&gt;
&lt;p&gt;If the zeros are real, then trying to &amp;ldquo;fill them in&amp;rdquo; isn&amp;rsquo;t just unnecessary; it&amp;rsquo;s actively destroying information. This gives me much more confidence in using models that work directly with the raw count data, like negative binomial generalized linear models (GLMs). These models are statistically appropriate, more interpretable, and keep us closer to the ground truth of the experiment.&lt;/p&gt;
&lt;p&gt;For my work in leukemia chemoresistance, this is critical. A gene being truly &amp;ldquo;off&amp;rdquo; in a drug-resistant subclone versus &amp;ldquo;on&amp;rdquo; in a sensitive one is a powerful piece of causal evidence. It&amp;rsquo;s a signal I want to model directly, not a technical error to be smoothed over by an imputation algorithm.&lt;/p&gt;
&lt;h2 id=&#34;untapped-potential-and-the-road-ahead&#34;&gt;Untapped Potential and the Road Ahead&lt;/h2&gt;
&lt;p&gt;This work clarifies our thinking, but also points to new directions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;My Next Computational Step:&lt;/strong&gt; The immediate implication is to challenge my own preprocessing pipelines. I plan to systematically benchmark how much imputation actually &lt;em&gt;hurts&lt;/em&gt; the ability to identify the effects of genetic perturbations in my Perturb-seq data. My hypothesis, strengthened by this paper, is that for detecting the strong, often binary on/off gene expression changes induced by CRISPR guides, working directly with the counts will be more powerful and robust than working with imputed data. I&amp;rsquo;ll design this benchmark using my existing T-ALL datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Experiment for the Field:&lt;/strong&gt; The author correctly points out that there&amp;rsquo;s a lack of comparable negative control data for plate-based scRNA-seq methods. To truly settle this debate across technologies, the community needs a gold-standard &amp;ldquo;zero-free&amp;rdquo; control experiment for a popular plate-based method like SMART-seq3. This would allow for a definitive, apples-to-apples comparison of the noise profiles and tell us if those platforms &lt;em&gt;do&lt;/em&gt; have unique technical artifacts that might warrant a different statistical approach.&lt;/p&gt;
&lt;h2 id=&#34;a-healthy-dose-of-skepticism&#34;&gt;A Healthy Dose of Skepticism&lt;/h2&gt;
&lt;p&gt;My main critique is that as a &amp;ldquo;correspondence&amp;rdquo; piece, the analysis is necessarily brief. The conclusion—&amp;ldquo;Droplet scRNA-seq is not zero-inflated&amp;rdquo;—is very broad, but the evidence is strongest for UMI-based droplet platforms. The argument against plate-based methods is less direct, relying on re-analysis of a single dataset and another group&amp;rsquo;s simulation study. While the evidence presented for droplet methods is compelling, I&amp;rsquo;d be cautious about over-generalizing this to &lt;em&gt;all&lt;/em&gt; scRNA-seq technologies without more direct, controlled comparisons. The core message is a crucial course correction for the field, but the nuanced differences between technologies still matter.&lt;/p&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://doi.org/10.1038/s41587-019-0379-5&#34;&gt;&lt;strong&gt;Droplet scRNA-seq is not zero-inflated&lt;/strong&gt;&lt;/a&gt; Nature Biotechnology (2020), by Valentine Svensson&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
