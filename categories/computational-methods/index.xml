<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computational Methods | Qingzhou Zhang</title>
    <link>https://zqzneptune.github.io/categories/computational-methods/</link>
      <atom:link href="https://zqzneptune.github.io/categories/computational-methods/index.xml" rel="self" type="application/rss+xml" />
    <description>Computational Methods</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2014-2025 Qingzhou Zhang (CC) BY-NC-SA 4.0. All thoughts and opinions here are my own.</copyright><lastBuildDate>Fri, 29 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://zqzneptune.github.io/images/icon_huf504e8a2f7c5cf9e43cec507892894a0_49847_512x512_fill_lanczos_center_2.png</url>
      <title>Computational Methods</title>
      <link>https://zqzneptune.github.io/categories/computational-methods/</link>
    </image>
    
    <item>
      <title>Tangram: Stitching the Cellular Map Back Together</title>
      <link>https://zqzneptune.github.io/post/2021-10-29-tangram/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://zqzneptune.github.io/post/2021-10-29-tangram/</guid>
      <description>&lt;h2 id=&#34;the-bottom-line-up-front&#34;&gt;The Bottom Line Up Front&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve just finished reading Biancalani et al.&amp;lsquo;s 2021 Nature Methods paper on &amp;ldquo;Tangram,&amp;rdquo; and it&amp;rsquo;s a piece of work that sits squarely in my wheelhouse. In essence, they&amp;rsquo;ve built a computational tool that solves a fundamental tradeoff in genomics: it takes the deep, genome-wide information from dissociated single-cell sequencing (scRNA-seq) and intelligently projects it back onto a spatial coordinate system derived from technologies like Visium, MERFISH, or even histological images.&lt;/p&gt;
&lt;h2 id=&#34;the-where-problem-in-single-cell-genomics&#34;&gt;The &amp;ldquo;Where&amp;rdquo; Problem in Single-Cell Genomics&lt;/h2&gt;
&lt;p&gt;The central challenge this paper tackles is one every computational biologist in this space wrestles with. We live in a world of powerful, but siloed, technologies. On one hand, scRNA-seq gives us an incredibly rich, transcriptome-wide view of individual cells, but to get it, we have to grind up the tissue, destroying all spatial context. We&amp;rsquo;re left with a &amp;lsquo;bag of cells&amp;rsquo;—we know &lt;em&gt;what&lt;/em&gt; is there, but not &lt;em&gt;where&lt;/em&gt;. On the other hand, spatial technologies like MERFISH or Visium tell us where genes are expressed, but they are either limited to a pre-selected panel of genes or have resolutions coarser than a single cell. The biological puzzle is clear: how do we get the best of both worlds? How do we put the rich molecular puzzle pieces from scRNA-seq back into their original spatial picture?&lt;/p&gt;
&lt;h2 id=&#34;under-the-hood-a-jigsaw-puzzle-solver-for-cells&#34;&gt;Under the Hood: A Jigsaw Puzzle Solver for Cells&lt;/h2&gt;
&lt;p&gt;Tangram’s approach is elegant. It&amp;rsquo;s a deep learning framework based on non-convex optimization that treats the problem like a massive jigsaw puzzle. The scRNA-seq profiles are the individual puzzle pieces, and the spatial data (e.g., a MERFISH slide) provides the outline or the image on the box. The algorithm computationally &amp;ldquo;places&amp;rdquo; each cell from the scRNA-seq data into a location on the spatial map. The model is trained by rewarding placements that maximize the similarity between the resulting in-silico tissue&amp;rsquo;s gene expression and the real spatial data&amp;rsquo;s gene expression. It iteratively shuffles the cell assignments until the total spatial correlation across all shared genes is maximized. The output is a probabilistic map, telling you the likelihood of finding each specific cell from your single-cell dataset at every voxel of the spatial dataset.&lt;/p&gt;
&lt;h2 id=&#34;beyond-mapping-imputation-as-a-superpower&#34;&gt;Beyond Mapping: Imputation as a Superpower&lt;/h2&gt;
&lt;p&gt;Here’s the finding that made me lean in. Tangram is more than just a cell-type mapping tool. Its real power lies in its ability to &lt;em&gt;impute&lt;/em&gt; and &lt;em&gt;predict&lt;/em&gt;. This is where it aligns perfectly with my mission of building predictive models.&lt;/p&gt;
&lt;p&gt;First, they show they can take a targeted MERFISH experiment measuring only ~250 genes and use it as a scaffold to project a full snRNA-seq dataset, effectively creating a genome-wide spatial map with ~27,000 genes at single-cell resolution. This is a massive leap in data generation without running a new experiment.&lt;/p&gt;
&lt;p&gt;Second, and even more compellingly, they map multi-omic SHARE-seq data (which profiles both RNA and chromatin accessibility in the same cell). By using the RNA modality as the &amp;ldquo;anchor&amp;rdquo; to align to the spatial MERFISH data, they successfully impute the spatial patterns of chromatin accessibility. This is a genuine act of prediction—projecting a data modality into a space where we currently have few tools to measure it directly. This transforms the tool from a descriptive aligner into a predictive engine.&lt;/p&gt;
&lt;h2 id=&#34;weaving-tangram-into-my-digital-twin-blueprint&#34;&gt;Weaving Tangram into My Digital Twin Blueprint&lt;/h2&gt;
&lt;p&gt;This work provides a foundational component for the predictive digital twins I aim to build. For my work in youth leukemia, it&amp;rsquo;s a game-changer. I can take a deep scRNA-seq dataset from a bone marrow aspirate—a classic &amp;lsquo;bag of cells&amp;rsquo;—and use Tangram to map those cells back onto a Visium slide from a patient&amp;rsquo;s bone marrow biopsy. This allows me to spatially resolve the locations of chemoresistant clones relative to supportive niches or immune cells. It&amp;rsquo;s the first step to modeling the spatial dependencies that drive drug resistance.&lt;/p&gt;
&lt;p&gt;The same logic applies directly to my solid tumor pillar. I can build high-resolution maps of the tumor microenvironment, placing specific T-cell exhaustion states or macrophage polarization states in the context of the tumor&amp;rsquo;s architecture. Tangram is the bridge between my perturbation-based causal discovery (from Perturb-seq) and the spatial reality of the tissue. It helps transform a list of cell states and gene programs into a spatially coherent, systems-level model.&lt;/p&gt;
&lt;h2 id=&#34;the-next-frontier-mapping-causality-in-space&#34;&gt;The Next Frontier: Mapping Causality in Space&lt;/h2&gt;
&lt;p&gt;This paper sets the stage for a critical next step, and it’s where I see my own research program making a unique contribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;My Next Computational Step:&lt;/strong&gt; The immediate opportunity is to integrate Perturb-seq data with spatial maps. My lab&amp;rsquo;s focus is on using CRISPR screens to build causal models of disease. The logical extension is to run a Perturb-seq screen on cancer cells, then use Tangram to map those perturbed cells back onto a spatial atlas of a tumor. This would allow me to ask questions like: if I knock out Gene X, which is critical for metastasis, where do those cells now localize in the tumor? How does that knockout causally influence the gene expression of its immediate neighbors? This fuses causal inference with spatial biology, moving us toward predictive models of how genetic perturbations rewire entire tissue ecosystems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Experiment for the Field:&lt;/strong&gt; The boldest claim in the paper is the imputation of spatial chromatin accessibility. This needs rigorous experimental validation. The key experiment would be to take a tissue block, perform MERFISH on one section, and use a next-generation spatial-ATAC technology on an adjacent serial section. This would create a ground-truth dataset to directly compare against Tangram&amp;rsquo;s imputed ATAC-seq patterns. Passing this test would be a powerful confirmation of the algorithm&amp;rsquo;s predictive capabilities across modalities.&lt;/p&gt;
&lt;h2 id=&#34;a-necessary-dose-of-skepticism&#34;&gt;A Necessary Dose of Skepticism&lt;/h2&gt;
&lt;p&gt;As powerful as this is, it&amp;rsquo;s important to recognize the inherent assumptions. The model&amp;rsquo;s accuracy is fundamentally capped by the quality of its inputs. It assumes the scRNA-seq dataset is a comprehensive representation of the cell populations present in the spatial slice. If a rare but critical cell type is absent from your single-cell prep due to sampling bias or dissociation artifacts—the &amp;lsquo;missing puzzle piece&amp;rsquo; scenario—Tangram has no way to place it. The model can&amp;rsquo;t invent what it hasn&amp;rsquo;t seen. Furthermore, its ability to deconvolve coarse technologies like Visium is powerful, but the placement of individual cells within a single 50-micron spot is a probabilistic estimation, not a direct measurement. We must be careful not to over-interpret the precision at the sub-spot level.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;Biancalani, Tommaso, et al. &lt;a href=&#34;https://www.nature.com/articles/s41592-021-01264-7&#34;&gt;&amp;ldquo;Deep learning and alignment of spatially resolved single-cell transcriptomes with Tangram.&amp;quot;&lt;/a&gt; Nature methods 18.11 (2021): 1352-1362.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Blueprint for Causal Biology: CROP-seq Links Gene to Function at Scale</title>
      <link>https://zqzneptune.github.io/post/2021-02-04_cropseqinit/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://zqzneptune.github.io/post/2021-02-04_cropseqinit/</guid>
      <description>&lt;h2 id=&#34;the-core-finding-in-a-nutshell&#34;&gt;The Core Finding in a Nutshell&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve been thinking a lot about the foundational papers that enable my entire research program, and this 2017 classic from Christoph Bock&amp;rsquo;s lab is undoubtedly one of them. Datlinger et al. present CROP-seq, a method that ingeniously combines pooled CRISPR screening with single-cell RNA sequencing. By engineering a lentiviral vector that makes the guide RNA (gRNA) itself detectable in a standard scRNA-seq workflow, they created a one-pot system to knock out thousands of different genes and read out their full transcriptomic consequences in thousands of individual cells.&lt;/p&gt;
&lt;h2 id=&#34;beyond-live-or-die-the-quest-for-richer-functional-screens&#34;&gt;Beyond &amp;lsquo;Live or Die&amp;rsquo;: The Quest for Richer Functional Screens&lt;/h2&gt;
&lt;p&gt;For years, functional genomics was caught between two paradigms. On one hand, you had pooled CRISPR screens—incredibly powerful and scalable, but limited to crude, binary readouts like cell survival or the expression of a single fluorescent reporter. They could tell you &lt;em&gt;if&lt;/em&gt; a gene was important, but not &lt;em&gt;why&lt;/em&gt;. On the other hand, you had arrayed screens, where you&amp;rsquo;d knock out one gene per well. This allowed for rich readouts like RNA-seq, but it was painfully low-throughput and expensive. The central challenge was clear: how do we get the rich, molecular-level data of arrayed screens at the scale of a pooled screen? How do we move beyond simple correlation to build a causal, predictive understanding of gene function?&lt;/p&gt;
&lt;h2 id=&#34;the-elegant-hack-making-grnas-visible-to-the-sequencer&#34;&gt;The Elegant Hack: Making gRNAs Visible to the Sequencer&lt;/h2&gt;
&lt;p&gt;The technical barrier was that gRNAs are transcribed by RNA Polymerase III and lack the poly-A tails needed for capture by the oligo-dT primers used in most scRNA-seq protocols. The CROP-seq vector is the solution, and it’s a beautiful piece of molecular engineering. The authors placed the entire gRNA expression cassette within the 3&amp;rsquo; Long Terminal Repeat (LTR) of a lentiviral vector. A quirk of lentiviral replication is that this 3&amp;rsquo; LTR is duplicated to the 5&amp;rsquo; end upon integration into the host genome. The result is that the gRNA sequence becomes part of a polyadenylated RNA Polymerase II transcript, making it perfectly visible to the sequencer alongside the cell&amp;rsquo;s own mRNAs. Critically, the original cassette still expresses a functional gRNA to direct Cas9. This allows the core computational step: in a massive pool of sequenced cells, we can now unambiguously assign a specific genetic perturbation (the gRNA) to a specific phenotypic outcome (the single-cell transcriptome).&lt;/p&gt;
&lt;h2 id=&#34;the-barcode-is-the-message&#34;&gt;The Barcode Is the Message&lt;/h2&gt;
&lt;p&gt;For me, the most profound insight of this paper is its design simplicity. While concurrent methods like the initial version of Perturb-seq used a separate transcribed barcode, CROP-seq makes the gRNA sequence &lt;em&gt;itself&lt;/em&gt; the identifier. This is not a minor detail. It means the method is fully compatible with standard, widely available pooled gRNA libraries and cloning protocols. By making the perturbing agent its own detectable barcode, they dramatically lowered the barrier to entry and created a system built for massive scale. This is the kind of thinking that truly democratizes a technology and unlocks its full potential.&lt;/p&gt;
&lt;h2 id=&#34;a-foundational-tool-for-my-predictive-modeling-mission&#34;&gt;A Foundational Tool for My Predictive Modeling Mission&lt;/h2&gt;
&lt;p&gt;This paper, along with the seminal Perturb-seq papers from the Regev and Weissman labs published around the same time, provides the technological bedrock for my entire mission to build predictive models of living tissues. My goal is to move from descriptive snapshots to causal, predictive digital twins. CROP-seq is the engine that produces the exact kind of data I need: high-throughput, high-dimensional measurements of causal relationships between genotype (the knockout) and phenotype (the transcriptome).&lt;/p&gt;
&lt;p&gt;The authors&amp;rsquo; proof-of-concept screen of T-cell receptor signaling in Jurkat cells—a T-ALL cell line—is a direct signpost for my primary research pillar. I can immediately see the path to applying this exact framework to dissect the gene regulatory networks that drive chemoresistance in youth leukemia. By perturbing genes and observing the resulting transcriptomic shifts in the face of chemotherapy, we can build models that predict which pathways a cancer cell will use to evade treatment. This is the blueprint.&lt;/p&gt;
&lt;h2 id=&#34;from-pathways-to-networks-the-next-computational-frontier&#34;&gt;From Pathways to Networks: The Next Computational Frontier&lt;/h2&gt;
&lt;p&gt;This work opens the door, but the journey is just beginning. The analytical approach in the paper largely involves averaging the transcriptomes of all cells that received the same gRNA to create a bulk-like &amp;ldquo;signature.&amp;rdquo; This is powerful, but it discards the rich heterogeneity that is the whole point of single-cell analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;My Next Computational Step:&lt;/strong&gt; Instead of calculating an average perturbation effect, my focus is on modeling the full &lt;em&gt;distribution&lt;/em&gt; of cellular states that arise from a single knockout. Drug resistance is rarely a uniform phenomenon; it emerges from the tails of a distribution. By modeling how a perturbation reshapes this distribution, we can identify the rare, emergent cell states that lead to treatment failure. This data is the ideal fuel for training the interpretable Gene Regulatory Network (GRN) models that are central to my lab&amp;rsquo;s vision, allowing us to predict perturbation effects &lt;em&gt;in silico&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Experiment for the Field:&lt;/strong&gt; The next essential move is to take this technology from sterile cell culture plates into more complex, physiologically relevant systems. A pivotal experiment would be to perform a CROP-seq screen in a patient-derived xenograft (PDX) model of B-ALL during chemotherapy treatment. This would allow us to screen for drivers of resistance not in isolation, but within the context of a living tumor microenvironment, revealing the causal gene networks that matter in a patient.&lt;/p&gt;
&lt;h2 id=&#34;scrutinizing-the-signal-limitations-and-lookouts&#34;&gt;Scrutinizing the Signal: Limitations and Lookouts&lt;/h2&gt;
&lt;p&gt;Of course, no method is without its limitations. The primary challenge with this approach is statistical power. The transcriptomic effect of a single gene knockout can be subtle, especially for genes in redundant pathways. This means a large number of cells must be sequenced for each gRNA to confidently detect a signal, which has cost implications. Furthermore, the capture efficiency of the gRNA transcript isn&amp;rsquo;t perfect, leading to some cells where we get a transcriptome but can&amp;rsquo;t identify the perturbation. Finally, the reliance on lentivirus is a key constraint; it works beautifully in immortalized cell lines but can be challenging to implement in primary cells, particularly non-dividing ones like mature neurons, which is a consideration for my long-term neurodegeneration work. Acknowledging these practical hurdles is the first step in designing the next generation of experiments.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;Datlinger, Paul, et al. &lt;a href=&#34;https://www.nature.com/articles/nmeth.4177&#34;&gt;&amp;ldquo;Pooled CRISPR screening with single-cell transcriptome readout.&amp;quot;&lt;/a&gt; Nature methods 14.3 (2017): 297-301.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Busting the Myth of the scRNA-seq &#34;Dropout&#34;</title>
      <link>https://zqzneptune.github.io/post/2020-01-20-zerodrop/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://zqzneptune.github.io/post/2020-01-20-zerodrop/</guid>
      <description>&lt;h2 id=&#34;the-30-second-lowdown&#34;&gt;The 30-Second Lowdown&lt;/h2&gt;
&lt;p&gt;I love papers that challenge a core assumption we&amp;rsquo;ve all been working with. In this short but powerful correspondence, Valentine Svensson re-examines the widely held belief that droplet-based single-cell RNA-seq data has &amp;ldquo;too many&amp;rdquo; zeros due to technical failures, a phenomenon often called &amp;ldquo;dropout.&amp;rdquo; Using a series of clever analyses on negative control datasets, he demonstrates that the number of observed zeros is almost perfectly explained by standard statistical models of molecule counting. The punchline? The &amp;ldquo;excess&amp;rdquo; zeros we see in our biological data aren&amp;rsquo;t technical noise to be corrected; they are a reflection of real biology.&lt;/p&gt;
&lt;h2 id=&#34;the-ghost-in-the-machine-are-zeros-signal-or-noise&#34;&gt;The Ghost in the Machine: Are Zeros Signal or Noise?&lt;/h2&gt;
&lt;p&gt;A fundamental question in single-cell genomics has always been this: when my data matrix shows a zero for a given gene in a specific cell, what does it mean? Did the technology simply fail to capture an expressed transcript (a &amp;ldquo;technical zero&amp;rdquo;), or is that gene genuinely not being expressed (a &amp;ldquo;biological zero&amp;rdquo;)?&lt;/p&gt;
&lt;p&gt;This isn&amp;rsquo;t just an academic question—it dictates how we build our models. For years, the field has leaned heavily on the idea that technical zeros are rampant, especially in high-throughput droplet methods. This assumption has spawned an entire cottage industry of computational methods for &amp;ldquo;imputation&amp;rdquo;—algorithms designed to predict and fill in these supposed missing values. But if that foundational assumption is shaky, it means we might be spending our time &amp;ldquo;fixing&amp;rdquo; a problem that doesn&amp;rsquo;t exist, and potentially even corrupting the real biological signal in the process.&lt;/p&gt;
&lt;h2 id=&#34;a-clean-experiment-to-settle-the-score&#34;&gt;A Clean Experiment to Settle the Score&lt;/h2&gt;
&lt;p&gt;The beauty of this paper lies not in a complex new algorithm, but in its elegant application of statistical first principles to the right kind of data. Svensson leverages public negative control datasets where, instead of single cells, a uniform solution of RNA is encapsulated into droplets. This creates a perfect testbed: a system with zero biological variation, where any deviation from a simple sampling model must be technical.&lt;/p&gt;
&lt;p&gt;He then fits a standard count model—the negative binomial distribution—to this data. This model is a workhorse for count data and accounts for the fact that capturing molecules is a random process. The result is striking: the model perfectly predicts the observed fraction of zeros without needing any extra &amp;ldquo;zero-inflation&amp;rdquo; component. It’s a textbook case of Occam&amp;rsquo;s razor; the simplest model fits the technical data perfectly.&lt;/p&gt;
&lt;h2 id=&#34;the-finding-that-changes-things&#34;&gt;The Finding That Changes Things&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;aha!&amp;rsquo; moment for me is the stark visual contrast in Figure 1. When you look at the plots from the technical control experiments (panels a-e), the black dots (observed zero fraction) sit right on top of the gray line (the model&amp;rsquo;s prediction). There is no &amp;ldquo;excess&amp;rdquo; zero problem.&lt;/p&gt;
&lt;p&gt;Then, you look at the plots from real biological samples (panels f-h). Here, you see a clear deviation: many genes have more zeros than predicted by a simple model with a single &amp;ldquo;dispersion&amp;rdquo; parameter for all genes. This is the deviation that has fueled the dropout narrative. But the author shows that once you allow each gene to have its own biological variability (a gene-wise dispersion), the model once again explains the data much better. The takeaway is unambiguous: the platform itself isn&amp;rsquo;t systematically failing. The extra zeros come from biological heterogeneity. Some cells express a gene, others don&amp;rsquo;t. That’s biology, not a bug.&lt;/p&gt;
&lt;h2 id=&#34;why-this-matters-for-building-predictive-models&#34;&gt;Why This Matters for Building Predictive Models&lt;/h2&gt;
&lt;p&gt;This paper lands squarely on my intellectual home turf. My entire research program is geared towards building interpretable, predictive models of living tissues, and that starts with respecting the data. For years, we&amp;rsquo;ve seen increasingly complex imputation models, many of which are black boxes that fundamentally alter the raw counts before any biological questions are asked. This work provides strong evidence that this entire preprocessing step might be a misguided effort.&lt;/p&gt;
&lt;p&gt;If the zeros are real, then trying to &amp;ldquo;fill them in&amp;rdquo; isn&amp;rsquo;t just unnecessary; it&amp;rsquo;s actively destroying information. This gives me much more confidence in using models that work directly with the raw count data, like negative binomial generalized linear models (GLMs). These models are statistically appropriate, more interpretable, and keep us closer to the ground truth of the experiment.&lt;/p&gt;
&lt;p&gt;For my work in leukemia chemoresistance, this is critical. A gene being truly &amp;ldquo;off&amp;rdquo; in a drug-resistant subclone versus &amp;ldquo;on&amp;rdquo; in a sensitive one is a powerful piece of causal evidence. It&amp;rsquo;s a signal I want to model directly, not a technical error to be smoothed over by an imputation algorithm.&lt;/p&gt;
&lt;h2 id=&#34;untapped-potential-and-the-road-ahead&#34;&gt;Untapped Potential and the Road Ahead&lt;/h2&gt;
&lt;p&gt;This work clarifies our thinking, but also points to new directions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;My Next Computational Step:&lt;/strong&gt; The immediate implication is to challenge my own preprocessing pipelines. I plan to systematically benchmark how much imputation actually &lt;em&gt;hurts&lt;/em&gt; the ability to identify the effects of genetic perturbations in my Perturb-seq data. My hypothesis, strengthened by this paper, is that for detecting the strong, often binary on/off gene expression changes induced by CRISPR guides, working directly with the counts will be more powerful and robust than working with imputed data. I&amp;rsquo;ll design this benchmark using my existing T-ALL datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Experiment for the Field:&lt;/strong&gt; The author correctly points out that there&amp;rsquo;s a lack of comparable negative control data for plate-based scRNA-seq methods. To truly settle this debate across technologies, the community needs a gold-standard &amp;ldquo;zero-free&amp;rdquo; control experiment for a popular plate-based method like SMART-seq3. This would allow for a definitive, apples-to-apples comparison of the noise profiles and tell us if those platforms &lt;em&gt;do&lt;/em&gt; have unique technical artifacts that might warrant a different statistical approach.&lt;/p&gt;
&lt;h2 id=&#34;a-healthy-dose-of-skepticism&#34;&gt;A Healthy Dose of Skepticism&lt;/h2&gt;
&lt;p&gt;My main critique is that as a &amp;ldquo;correspondence&amp;rdquo; piece, the analysis is necessarily brief. The conclusion—&amp;ldquo;Droplet scRNA-seq is not zero-inflated&amp;rdquo;—is very broad, but the evidence is strongest for UMI-based droplet platforms. The argument against plate-based methods is less direct, relying on re-analysis of a single dataset and another group&amp;rsquo;s simulation study. While the evidence presented for droplet methods is compelling, I&amp;rsquo;d be cautious about over-generalizing this to &lt;em&gt;all&lt;/em&gt; scRNA-seq technologies without more direct, controlled comparisons. The core message is a crucial course correction for the field, but the nuanced differences between technologies still matter.&lt;/p&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://doi.org/10.1038/s41587-019-0379-5&#34;&gt;&lt;strong&gt;Droplet scRNA-seq is not zero-inflated&lt;/strong&gt;&lt;/a&gt; Nature Biotechnology (2020), by Valentine Svensson&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
